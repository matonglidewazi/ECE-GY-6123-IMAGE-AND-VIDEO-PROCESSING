{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTER ASSIGNMENT 04\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net for image segmentation\n",
    "\n",
    "Download train, train_mask, test from the following link:\n",
    "\n",
    "https://www.dropbox.com/sh/e8j991bsd269fcq/AACARtYIoYnQaydahUlo22NFa?dl=0\n",
    "\n",
    "and extract them to the current directory.\n",
    "\n",
    "\n",
    "In class, we talked about U-net for image segmentation.\n",
    "\n",
    "This assignment is intended to \n",
    "- help you better understand the concept of U-net for image segmentation \n",
    "- help you get started with designing networks in pytorch including loading data, network design, loss function, training and testing.\n",
    "\n",
    "You should \n",
    " -  Implement the U-net of the following architechure.\n",
    " ![U-net](U-net_architecture.png)\n",
    " -  Write function dice_coeff(input, target) for evaluation\n",
    " -  Load training dataset and testing dataset.\n",
    " Notice that you should rescale the images to a smaller size. Otherwise it's impossible to train on cpu.\n",
    " -  Train your network for a few epochs.\n",
    " -  Test your model by feeding in a new image in testing dataset. Plot your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from optparse import OptionParser\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torch.autograd import Function, Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ TODO 1 ] First define following layers to be used later\n",
    "- **Conv2d + BatchNorm2d + ReLu ** as **single conv2d layer** ,\n",
    "- **Maxpool2d + single conv2d layer ** as **down layer**,\n",
    "- **Upsample + single conv2d layer ** as **up layer** ,\n",
    "-  **Conv2d ** as **outconv layer** \n",
    "\n",
    "You can check out the documentation in this link to understand how to use the methods called in the provided template:\n",
    "\n",
    " https://pytorch.org/docs/stable/nn.html\n",
    " \n",
    "  ![single_conv](single_conv_layer.png)\n",
    "  ![down_layer](down_layer.png)\n",
    "  ![up_layer](Up_layer.png)\n",
    "  ![outconv_layer](outconv_layer.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# DEFINE SINGLE_CONV CLASS\n",
    "class single_conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(single_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True) # Save memory\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# DEFINE DOWN CLASS\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.down = nn.MaxPool2d(kernel_size=2, stride=2) # use nn.MaxPool2d( )\n",
    "        self.conv = single_conv(in_ch, out_ch) # use previously defined single_cov\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# DEFINE UP CLASS\n",
    "# Note that this class will not only upsample x1, but also concatenate up-sampled x1 with x2 to generate the final output\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up, self).__init__()       \n",
    "        self.up = nn.Upsample(scale_factor=2) # use nn.Upsample( )\n",
    "        self.conv = single_conv(in_ch, out_ch) # use previously defined single_cov\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # This part is tricky, so we provide it for you\n",
    "        # First we upsample x1\n",
    "        x1 = self.up(x1)\n",
    "            \n",
    "        # Notice that x2 and x1 may not have the same spatial size. \n",
    "        # This is because when you downsample old_x2(say 25 by 25), you will get x1(12 by 12)   \n",
    "        # Then you perform upsample to x1, you will get new_x1(24 by 24)\n",
    "        # You should pad a new row and column so that new_x1 and x2 have the same size.\n",
    "        \n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "        # Now we concatenat x2 channels with x1 channels\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# DEFINE OUTCONV CLASS\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2D(in_ch, out_ch) # Use nn.Conv2D( ) since we do not need to do batch norm and relu at this layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# Build your network with predefined classes: single_conv, up, down, outconv\n",
    "import torch.nn.functional as F\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = single_conv(n_channels, 16) # conv2d +  batchnorm + relu\n",
    "        self.down1 = down(16, 32)         # maxpool2d + conv2d + batchnorm + relu\n",
    "        self.down2 = down(32, 32)         # maxpool2d + conv2d + batchnorm + relu\n",
    "\n",
    "        self.up1 = up(64, 16)             # upsample + pad + conv2d + batchnorm + relu\n",
    "        self.up2 = up(32, 16)             # upsample + pad + conv2d + batchnorm + relu\n",
    "\n",
    "        self.outc = outconv(16, 1)        # conv2d\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "\n",
    "        x = self.up1(x3, x2)\n",
    "        x = self.up2(x, x1)\n",
    "\n",
    "        x = self.outc(x)\n",
    "        return F.sigmoid(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ TODO 2 ] Define evaulation function:\n",
    "Based on what we have learnt in class, Dice coefficient is defined as \n",
    "![dice.png](dice.png)\n",
    "For the case of evaluating a Dice coefficient on predicted segmentation masks, we can approximate intersection of A and B as the element-wise multiplication between the prediction and target mask, and then sum the resulting matrix.\n",
    "\n",
    "In order to quantify the area of A and B, some researchers use the simple sum whereas other researchers prefer to use the squared sum for this calculation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# define dice coefficient \n",
    "class DiceCoeff(Function):\n",
    "    \"\"\"Dice coeff for one pair of input image and target image\"\"\"\n",
    "    def forward(self, input, target):\n",
    "        self.save_for_backward(input, target)\n",
    "        eps = 0.0001 # in case union = 0\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # Calculate intersection and union. \n",
    "        # You can convert the input image into a vector with input.contiguous().view(-1)\n",
    "        # Then use torch.dot(A, B) to calculate the intersection.\n",
    "        # Use torch.sum(A) to get the sum.\n",
    "        self.inter = torch.sum(input*target) # Instruction looks strange\n",
    "        self.union = torch.sum(input) + torch.sum(target) + eps\n",
    "        # Calculate DICE \n",
    "        d = self.inter/self.union\n",
    "        return d\n",
    "\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# Calculate dice coefficients for batches\n",
    "def dice_coeff(input, target):\n",
    "    \"\"\"Dice coeff for batches\"\"\"\n",
    "    s = torch.FloatTensor(1).zero_()\n",
    "    \n",
    "    # Are you sure this has grade or just for test?\n",
    "    \n",
    "    # For each pair of input and target, call DiceCoeff().forward(input, target) to calculate dice coefficient\n",
    "    # Then average\n",
    "    for i, c in enumerate(zip(input, target)):\n",
    "        s = s + DiceCoeff.forward(c[0], c[1]) \n",
    "    s = s / (i + 1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data ids. \n",
    "\n",
    "Split them into training and validation. Validation percent of 0.05 means 5% of training dataset is used as validation.\n",
    "\n",
    "You can try different percentage.\n",
    "\n",
    "This part has been done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(dir):\n",
    "    \"\"\"Returns a list of the ids in the directory\"\"\"\n",
    "    return (f[:-4] for f in os.listdir(dir))\n",
    "\n",
    "\n",
    "def split_ids(ids, n=2):\n",
    "    \"\"\"Split each id in n, creating n tuples (id, k) for each id\"\"\"\n",
    "    return ((id, i)  for id in ids for i in range(n))\n",
    "\n",
    "\n",
    "def split_train_val(dataset, val_percent=0.05):\n",
    "    \n",
    "    dataset = list(dataset)\n",
    "    length = len(dataset)\n",
    "    n = int(length * val_percent)\n",
    "    random.shuffle(dataset)\n",
    "    return {'train': dataset[:-n], 'val': dataset[-n:]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_img = 'train/'\n",
    "dir_mask = 'train_masks/'\n",
    "dir_checkpoint = 'checkpoints/'\n",
    "\n",
    "# Get ids of training dataset\n",
    "ids = get_ids(dir_img)\n",
    "ids = split_ids(ids)\n",
    "# iddataset consists iddataset['train'] and iddataset['val']\n",
    "# you can get all the ids of the images in training dataset with following code:\n",
    "# for id, pos in iddataset['train']:\n",
    "#    print(id)\n",
    "# you will need this in the following get_imgs_and_masks() function\n",
    "# Or you can also load images in your way\n",
    "iddataset = split_train_val(ids, 0.05)\n",
    "# Get the number of training samples\n",
    "N_train = len(iddataset['train'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ TODO 3 ] & [ TODO 4 ] Load images and start training your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You might need to use these functions in the following steps\n",
    "# hwc_to_chw: Convert images from Height*Width*Channels to Channels*Height*Width\n",
    "def hwc_to_chw(img):\n",
    "    return np.transpose(img, axes=[2, 0, 1])\n",
    "\n",
    "# normalize: normalize from 255 to 1\n",
    "def normalize(x):\n",
    "    return x / 255\n",
    "\n",
    "# batch:  Yields lists by batch\n",
    "def batch(iterable, batch_size):\n",
    "    b = []\n",
    "    for i, t in enumerate(iterable):\n",
    "        b.append(t)\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            yield b\n",
    "            b = []\n",
    "    if len(b) > 0:\n",
    "        yield b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# This function returns rescaled images and masks\n",
    "# Note that:\n",
    "#  - The shape of images should be Channels*rescaled_Height*rescaled_Width \n",
    "#  - Pixel values should be normalized to [0,1]\n",
    "def get_imgs_and_masks(ids, dir_img, dir_mask, scale):\n",
    "    \"\"\"Return all the couples (img, mask)\"\"\"\n",
    "    # Read in the image and rescale it according to the scale factor\n",
    "    # You can use Image.open() to read images\n",
    "    # Image is a package of PIL. Check https://pillow.readthedocs.io/en/stable/reference/Image.html for more details\n",
    "    imgs = ...\n",
    "    rescaled_imgs = ...\n",
    "\n",
    "    # Convert images from Height*Width*Channels to Channels*Height*Width\n",
    "    # you can use hwc_to_chw() \n",
    "    imgs_switched = ...\n",
    "    # Then normalize switched images to [0,1]\n",
    "    # you can use normalize()\n",
    "    imgs_normalized = ...\n",
    "    # Read in the mask and rescale it according to the scale factor\n",
    "    masks = ...\n",
    "    rescaled_masks = ...\n",
    "\n",
    "    return zip(imgs_normalized, rescaled_masks)\n",
    "################################################ [TODO] ###################################################\n",
    "# This function is used to evaluate the network after each epoch of training\n",
    "# Input: network and validation dataset\n",
    "# Output: average dice_coeff\n",
    "def eval_net(net, dataset):\n",
    "    # set net mode to evaluation\n",
    "    net.eval()\n",
    "    tot = 0\n",
    "    for i, b in enumerate(dataset):\n",
    "        img = b[0]\n",
    "        true_mask = b[1]\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # convert numpy array img and true_mask to torch tensor\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        true_mask = ...\n",
    "      \n",
    "        # Feed in the image to get predicted mask\n",
    "        mask_pred = net( )...\n",
    "        # For all pixels in predicted mask, set them to 1 if larger than 0.5. Otherwise set them to 0\n",
    "        mask_pred = ...\n",
    "        # calculate dice_coeff()\n",
    "        # note that you should add all the dice_coeff in validation/testing dataset together \n",
    "        # call dice_coeff() here\n",
    "        tot += ...\n",
    "        # Return average dice_coeff()\n",
    "    return tot / (i + 1)\n",
    "\n",
    "\n",
    "# convert numpy array img and true_mask to torch tensor\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        true_mask = torch.from_numpy(true_mask).unsqueeze(0)\n",
    "        # Feed in the image to get predicted mask\n",
    "        mask_pred = net(img)[0]\n",
    "        # For all pixels in predicted mask, set them to 1 if larger than 0.5. Otherwise set them to 0\n",
    "        mask_pred = (mask_pred > 0.5).float()\n",
    "        # calculate dice_coeff()\n",
    "        # note that you should add all the dice_coeff in validation/testing dataset together \n",
    "        tot += dice_coeff(mask_pred, true_mask).item()\n",
    "        # Return average dice_coeff()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# Create a UNET object. Input channels = 3, output channels = 1\n",
    "net = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# Specify number of epochs, image scale factor, batch size and learning rate\n",
    "epochs = ... # i.e, 10\n",
    "img_scale = ... # i.e, 1/16\n",
    "batch_size = ...# i.e, 50\n",
    "lr = ...        # i.e, 0.01\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# Define an optimizer for your model.\n",
    "# Pytorch has built-in package called optim. Most commonly used methods are already supported.\n",
    "# Here we use stochastic gradient descent to optimize\n",
    "# For usage of SGD, you can read https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html\n",
    "# Also you can use ADAM as the optimizer\n",
    "# For usage of ADAM, you can read https://www.programcreek.com/python/example/92667/torch.optim.Adam\n",
    "\n",
    "optimizer = optim.SGD(...)\n",
    "#OR optimizer = optim.Adam(...)\n",
    "\n",
    "\n",
    "\n",
    "#suggested parameter settings: momentum=0.9, weight_decay=0.0005\n",
    "\n",
    "# The loss function we use is binary cross entropy: nn.BCELoss()\n",
    "criterion = nn.BCELoss()\n",
    "# note that although we want to use DICE for evaluation, we use BCELoss for training in this example\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# Start training\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}/{}.'.format(epoch + 1, epochs))\n",
    "    net.train()\n",
    "    ################################################ [TODO] ###################################################\n",
    "    # Load images and masks for training and validation\n",
    "    train = get_imgs_and_masks(iddataset['train'], dir_img, dir_mask, img_scale)\n",
    "    val = get_imgs_and_masks(iddataset['val'], dir_img, dir_mask, img_scale)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, b in enumerate(batch(train, batch_size)):\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # Get images and masks from each batch\n",
    "        imgs = ...\n",
    "        true_masks = ...\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # Convert images and masks from numpy to torch tensor with torch.from_numpy\n",
    "        imgs = ...\n",
    "        true_masks = ...\n",
    "\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # Feed your images into the network\n",
    "        masks_pred = ...\n",
    "        # Flatten the predicted masks and true masks. For example, A_flat = A.view(-1)\n",
    "        masks_probs_flat = ...\n",
    "        true_masks_flat = ...\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # Calculate the loss by comparing the predicted masks vector and true masks vector\n",
    "        # And sum the losses together \n",
    "        loss = criterion(...)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        print('{0:.4f} --- loss: {1:.6f}'.format(i * batch_size / N_train, loss.item()))\n",
    "\n",
    "        # optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. \n",
    "        # It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n",
    "        optimizer.zero_grad()\n",
    "        # loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. \n",
    "        # These are accumulated into x.grad for every parameter x\n",
    "        # x.grad += dloss/dx\n",
    "        loss.backward()\n",
    "        # optimizer.step updates the value of x using the gradient x.grad. \n",
    "        # x += -lr * x.grad\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch finished ! Loss: {}'.format(epoch_loss / i))\n",
    "    ################################################ [TODO] ###################################################\n",
    "    # Perform validation with eval_net()\n",
    "    val_dice = ...\n",
    "    print('Validation Dice Coeff: {}'.format(val_dice))\n",
    "    # Save the model after each epoch\n",
    "    torch.save(net.state_dict(),\n",
    "                dir_checkpoint + 'CP{}.pth'.format(epoch + 1))\n",
    "    print('Checkpoint {} saved !'.format(epoch + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ TODO 5 ] load one image from testing dataset and plot output mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# Define a function for prediction/testing\n",
    "def predict_img(net,\n",
    "                img,\n",
    "                scale_factor=0.5,\n",
    "                out_threshold=0.5):\n",
    "    # set the mode of your network to evaluation\n",
    "    net.eval()\n",
    "    ################################################ [TODO] ###################################################\n",
    "    # get the height and width of your image\n",
    "    img_height = ...\n",
    "    img_width = ...\n",
    "    ################################################ [TODO] ###################################################\n",
    "    # resize the image according to the scale factor\n",
    "    img = ...\n",
    "    # Normalize the image by dividing by 255\n",
    "    img = ...\n",
    "    # convert from Height*Width*Channel TO Channel*Height*Width\n",
    "    img = ...\n",
    "    # convert numpy array to torch tensor \n",
    "    X_img = ...\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ################################################ [TODO] ###################################################\n",
    "        # predict the masks \n",
    "        output_img = ...\n",
    "        out_probs = output_img.squeeze(0)\n",
    "        # Rescale to its original size\n",
    "        out_probs = ...\n",
    "        # convert to numpy array\n",
    "        out_mask_np = ...\n",
    "\n",
    "    # For all pixels in predicted mask, set them to 1 if larger than 0.5. Otherwise set them to 0\n",
    "    return out_mask_np > out_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# Load an image from testing dataset\n",
    "test_img = ...\n",
    "    \n",
    "################################################ [TODO] ###################################################\n",
    "# Predict the mask\n",
    "mask = predict_img(net=net,\n",
    "                    img=test_img,\n",
    "                    scale_factor=...,\n",
    "                    out_threshold=...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot original image and mask image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot original images and masks\n",
    "...\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IVP",
   "language": "python",
   "name": "cs231"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
