{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import convolve as convolveim\n",
    "import scipy.ndimage.filters as filters\n",
    "import scipy.ndimage as ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A - Harris detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write your own program for Harris corner point detection at a fixed scale. Your program can contain the following steps:\n",
    " -  Generate gradient images of Ix and Iy using filters corresponding to derivative of Gaussian functions of a chosen **scale σ and window size w** (let us use w=4σ+1). You can use the conv2( ) function.\n",
    " -  Compute three images **Ix* Ix ,Iy* Iy , Ix* Iy**.\n",
    " -  To determine the Harris value at each pixel, we should apply Gaussian weighting over a window size of WxW centered at this pixel to each of the image Ix^2, Iy^2, and Ix Iy, and then sum the weighted average. This is equivalent to convolve each of these images by a Gaussian filter with size WxW. Let us use a Gaussian filter with scale 2σ, and window size W=6σ+1.\n",
    " -  Generate an image of Harris cornerness value by forming the moment matrix A at every pixel based on the images from (c).\n",
    " -  Detect local maxima in the above image (for simplicity you could just check whether a center pixel is larger than its 8 neighbors). **Pick the first N feature points with largest Harris values.**\n",
    " -  Mark each detected point using a small circle. Display the image with the detected points.\n",
    " -  You should write your own functions to generate Guassian and derivative of Gaussian filters from the analytical forms.\n",
    "- Apply your Harris detector to a test image (you can just work on gray scale image). Using σ=1, N=50. Do the features detected make sense?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss(siz, sigma):\n",
    "    tmp = int(siz / 2)\n",
    "    ################################################ TODO ###############################################\n",
    "    # define the x range for even and odd length\n",
    "    # Note: for even length -> Ex: size=6, you would have -2.5, -1.5, -0.5, 0.5, 1.5, 2.5\n",
    "#           for odd length ->  Ex: size=7, you would have -3,-2,......2,3\n",
    "    if np.mod(siz,2)==0:\n",
    "        ... # for even size\n",
    "    else:\n",
    "        ... # for odd size\n",
    "        \n",
    "    gauss = ... # make 1D gaussian filter\n",
    "    gauss2 = ... # Compose 2D Gaussian filter from 1D\n",
    "    gauss1 = ... # Normalize the filter so that all coefficients sum to 1\n",
    "    gauss1_dx = np.matrix(np.zeros((np.shape(gauss1)), dtype=\"float32\"))\n",
    "    gauss1_dy = np.matrix(np.zeros((np.shape(gauss1)), dtype=\"float32\"))\n",
    "\n",
    "    for j in range(0, len(x)):\n",
    "        gauss1_dx[:, j] = gauss1[:, j] * -x[j]/(sigma*sigma) # derivative filter in x\n",
    "        ################################################ TODO ###############################################\n",
    "        gauss1_dy[j, :] = ... # similarly define the difference in y\n",
    "    \n",
    "    \n",
    "    ################################################ TODO ###############################################\n",
    "    # Visualize the filters you created to make sure you are working with the correct filters\n",
    "    \n",
    "    return gauss1,gauss1_dx, gauss1_dy\n",
    "\n",
    "def harris(Ix, Iy ,N):\n",
    "\n",
    "    l, m = np.shape(input_image)\n",
    "    \n",
    "\n",
    "\n",
    "    ################################################ TODO ###############################################\n",
    "    #forming 3 images\n",
    "    Ix2 = ... #Ix square\n",
    "    Iy2 = ... #Iy square\n",
    "    Ixy = ... #Ix*Iy\n",
    "\n",
    "    # smoothing image Ix2, Iy2, Ixy  with Gaussian filter with sigma=2, size=7\n",
    "    gauss_smooth, _, _ = ...\n",
    "    Ix_smooth = convolveim(Ix2, gauss_smooth, mode='constant')\n",
    "    ################################################ TODO ###############################################\n",
    "    Iy_smooth = ... # CONVOLVE as shown above\n",
    "    Ixy_smooth = ...\n",
    "\n",
    "    H = np.zeros((np.shape(input_image)), dtype='float')\n",
    "    \n",
    "    ################################################ TODO ###############################################\n",
    "    # write code segment to find N harris points in the image\n",
    "    ...\n",
    "    ...\n",
    "    ...    \n",
    "    # x,y should be lists of x and y coordinates of the harris points.\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ TODO ###############################################\n",
    "input_image = ... # input image\n",
    "l,m = ... # image shape\n",
    "\n",
    "img = cv2.normalize(input_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "# Generating the gaussian filter\n",
    "sigma = 1\n",
    "size = 4*sigma + 1\n",
    "gauss_filt, gauss_filt_dx, gauss_filt_dy = ... # function call to gauss\n",
    "\n",
    "    \n",
    "################################################ TODO ###############################################\n",
    "#Convolving the filter with the image\n",
    "Ix = ... # convolve image with dx filter\n",
    "Iy = ... # convolve image with dy filter\n",
    "\n",
    "x,y = harris(Ix, Iy ,50)\n",
    "################################################ TODO ###############################################\n",
    "# plot the image with harris points\n",
    "# Hint: you may use \"plt.plot(x,y, 'ro')\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART B - SIFT descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program that can generate SIFT descriptor for each detected feature point using your program in Prob. 1. You may follow the following steps:\n",
    "- Generate gradient images Ix and Iy as before. Furthermore, determine the gradient magnitude and orientation from Ix and Iy at every pixel.\n",
    "-  Quantize the orientation of each pixel to one of the N=8 bins. Suppose your original orientation is x. To quantize the entire range of 360 degree to 8 bins, the bin size is q=360/N=45 degree. Assuming your orientation is determined with a range of [0,360]. You can perform quantization using\n",
    " - (a) x_q=floor((x+q/2)/q);ifx_q=N,x_q=0\n",
    " - (b) x_q will range from 0 to 7,with 0 corresponding to degree in(-22.5,22.5), or (0, 22.5) and (360-22.5, 360).\n",
    " - (c) Then for each detected feature point, you may follow the following steps to generate the SIFT descriptor:\n",
    "    - i) Generate a patch of size 16x16 centered at the detected feature point;\n",
    "    - ii) Multiply the gradient magnitude with a Guassian window with scale= patch width/2. \n",
    "    - iii) Generate a HoG for the entire patch using the weighted gradient magnitude.\n",
    "    - iv) Determine the dominant orientation of the patch by detecting the peak in the Hog determined in (e).\n",
    "    - v) Generate a HoG for each of the 4x4 cell in the 16x16 patch.\n",
    "    - vi) Shift each HoG so that the dominant orientation becomes the first bin. \n",
    "    - vii) Concatenate the HoG for all 16 cells into a single vector.\n",
    "    - viii) Normalize the vector. That is, divide each entry by L2 norm of the vector. \n",
    "    - ix) Clip the normalized vector so that entries >0.2 is set to 0.2.\n",
    "    - x) Renormalize the vector resulting from (k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histo(theta4,mag4):\n",
    "\n",
    "    temp = np.zeros((1,8),dtype='float32')\n",
    "    ################################################ TODO ###############################################\n",
    "    # write code segment to add the magnitudes of all vectors in same orientations\n",
    "    \n",
    "    # temp should be a 1x8 vector, where each value corresponds to an orientation and \n",
    "    # contains the sum of all gradient magnitude, oriented in that orientation\n",
    "    return temp\n",
    "\n",
    "def descriptor(theta16,mag16):\n",
    "    filt,_,_ = gauss(16,8) \n",
    "    mag16_filt = np.multiply(mag16, filt) \n",
    "\n",
    "    desp = np.array([])\n",
    "    ################################################ TODO ###############################################\n",
    "    histo16 = ... # make function call to histo, with arguments theta16 and mag16_fil\n",
    "    maxloc_theta16 = np.argmax(histo16)\n",
    "\n",
    "    for i in range(0,16,4):\n",
    "        for j in range(0,16,4):\n",
    "            ################################################ TODO ###############################################\n",
    "            # use histo function to create histogram of oriantations on 4x4 pathces in the neighbourhood of the harris points\n",
    "            # you should shift your histogram for each cell so that the dominant orientation of the 16x16 patch becomes the first quantized orientation\n",
    "            # shifting can be done using np.roll( )\n",
    "            # you should update the variable desp to store all the orientation magnitude sum for each sub region of size 4x4\n",
    "            \n",
    "\n",
    "    ################################################ TODO ###############################################\n",
    "    # clip the descriptors after normalization\n",
    "\n",
    "    desp = desp / np.linalg.norm(desp, 2)\n",
    "    desp = np.matrix(desp)\n",
    "\n",
    "    return desp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_B(input_image):\n",
    "    p, q = ... # shape of input image\n",
    "\n",
    "    # normalize the image\n",
    "    img = ...\n",
    "\n",
    "    # Generate derivative of Gaussian filters, using sigma=1, filter window size=4*sigma+1\n",
    "    _, filt_x, filt_y = ...\n",
    "\n",
    "    ################################################ TODO ###############################################\n",
    "    img_x = ... # image convolved with filt_x\n",
    "    img_y = ... # image convolved with filt_y\n",
    "\n",
    "    mag = np.sqrt(img_x ** 2 + img_y ** 2)\n",
    "    theta = np.arctan2(img_x, img_y) + np.pi\n",
    "\n",
    "    for x in range(0, p):\n",
    "        for y in range(0, q):\n",
    "            temp = theta[x, y]\n",
    "            if (temp < np.pi / 8 and temp >= 0) or (temp <= 2 * np.pi and temp >= (2 * np.pi - np.pi / 8)):\n",
    "                theta[x, y] = 0\n",
    "            ################################################ TODO ###############################################\n",
    "            # fill in the if condition for all other angles\n",
    "\n",
    "\n",
    "    ################################################ TODO ###############################################\n",
    "    x,y = ... # call harris function to find 50 feature points \n",
    "    \n",
    "    # Adding 15 rows and columns. You will need this extra border to get a patch centered at the feature point \n",
    "    #    when the feature points lie on the original border of the image.\n",
    "    theta = cv2.copyMakeBorder(theta, 7,8,7,8, cv2.BORDER_REFLECT, 0) # check the function definition for cv2.copyMakeBorder\n",
    "    ################################################ TODO ###############################################\n",
    "    mag = ... # similarly add border to the magnitude image\n",
    "    final_descriptor = np.zeros((1,128))\n",
    "    final_points = np.transpose(np.matrix([x,y]))\n",
    "\n",
    "    for i in range(0,len(x)):\n",
    "        # Since you have already added 15 rows and columns, now the new coordinates of the feature points are (x+8, y+8).\n",
    "        # Then the patch should be [x[i]:x[i]+16,y[i]:y[i]16]\n",
    "        theta_temp = theta[x[i]:x[i]+16,y[i]:y[i]16] # Your patch should be centered at the feature point!\n",
    "        mag_temp = ... # similarly, take a 16x16 patch of mag around the point\n",
    "        temp2 = ... # function call to descriptors\n",
    "        final_descriptor = np.append(final_descriptor,temp2,axis=0)\n",
    "\n",
    "    # Initially, final descriptor has a row of zeros. We are deleting that extra row here.\n",
    "    final_descriptor = np.delete(final_descriptor,0,0)\n",
    "    final_descriptor = np.nan_to_num(final_descriptor)\n",
    "    final_descriptor = np.array(final_descriptor)\n",
    "    \n",
    "    return final_descriptor,final_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ TODO ###############################################\n",
    "input_image = ... # input image\n",
    "\n",
    "final_descriptor , final_points = part_B(input_image)\n",
    "\n",
    "for i in range(len(x)):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(input_image,cmap='gray')\n",
    "    ax1.autoscale(False)\n",
    "    ax1.plot(x[i],y[i], 'ro')\n",
    "    ax2.bar(np.arange(1,129),final_descriptor[i,:])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART C - correspondance in 2 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding corresponding points in two images based on SIFT descriptors.\n",
    "-  Using your program in Part A and Part B to detect feature points and generate their descriptors for two images provided with this assignment.(image named **left** and **right**)\n",
    "-  Write a program that can find matching points between the two images. For each point p in the first image, you compute its distance to each point in the second image (using Euclidean distance between two SIFT descriptors) to find two closest points q1 and q2 in the second image. Let us call the distance of p to q1 and q2 by d1 and d2, you will take point q1 as the matching point for p if d1/d2 <r. Otherwise, you assume there is ambiguity between q1 and q2 and skip the feature point p in image 1. You can experiment with different threshold r. Obviously r should be <1. At the end of this process, you should have a set of matching pairs.\n",
    "-  Create an image that shows the matching results. For example you can create a large image that has the left and right images side by side, and draw lines between matching pairs in these two images. Do the matched points look reasonable? You can use **cv2.line()** to draw line between each matching pair. You need to save and display the image after you add lines into the image array using the cv2.line() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################ TODO ###############################################\n",
    "img1 = ... # read left image\n",
    "img_1 = cv2.normalize(img1, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "l,m = np.shape(img_1)\n",
    "\n",
    "descriptor_1, keypoints_1 = part_B(img_1)\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "# Generate a rotated image\n",
    "# plot the rotated image with harris points.\n",
    "\n",
    "img2 = ... # read right image\n",
    "img_2 = cv2.normalize(img2, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "descriptor_2, keypoints_2 = part_B(img_2)\n",
    "\n",
    "matched_loca = list() # list of all corresponding points pairs. Point pairs can be stored as tuples\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "# write code segment to find corresponding points in image\n",
    "\n",
    "\n",
    "final_image = np.concatenate((img_1,img_2),axis=1)\n",
    "print('number of corresponding poitnts found:',len(matched_loca))\n",
    "################################################ TODO ###############################################\n",
    "# write code segment to draw lines joining corresponding points\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(final_image,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART D - panorama stiching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stitch two images into a panorama using SIFT feature detector and descriptor.\n",
    "- Read in an image pair (left and right)\n",
    "- Detect SIFT points and extract SIFT features from each image by using the following OpenCV sample code. \n",
    "\n",
    "\n",
    "\n",
    "    - sift = cv2.xfeatures2d.SIFT_create()\n",
    "    - skp = sift.detect(img,None)\n",
    "    - (skp, features) = sift.compute(img, skp)\n",
    "\n",
    "\n",
    "\n",
    "[comment]: ![](q_img.png)\n",
    " - **Where skp is a list of all the key points found from img and sd is the descriptor for the image. Each element in skp is an OpenCV ‘key points class’ object, and you can check the corresponding coordinate by skp[element_index].pt **\n",
    "- Mark the detected points in each image by a circle with radius equal to the scale of the feature. You can check the scale of a detected point using skp[element_index].size\n",
    "- Find the corresponding point pairs between left and right images based on their SIFT descriptors. You can reuse your program for Part-C.\n",
    "- Apply RANSAC method to these matching pairs to find the largest subset of matching pairs that are related by the same homography. You can use the function cv2.findHomography(srcPoints, dstPoints, cv2.RANSAC)\n",
    "- Create an image that shows the matching results by drawing lines between corresponding points. You can reuse the plotting function you developed for Part-C.\n",
    "- Apply the homography determined in (e) to the right image. You can use cv2.warpPerspective() to apply the homography transformation to the image.\n",
    "- Stitch the transformed image from (g) and the right image together to generate the panorama.\n",
    "\n",
    "_Hint: you should first create am array for the stitched image whose width is larger of the width of transformed left image and the right image. Similarly, the height is the larger of the heights of the transformed left image and the right image. Then, you put transformed left image into this array, and then the original right image into this array. With this simple approach, the part where the two images overlap will be represented by the right image._\n",
    "\n",
    "**In your report, show the left and right images, the left and right images with SIFT points indicated, the image that illustrates the matching line between corresponding points, the transformed left image, and finally the stitched image.**\n",
    "\n",
    "**Also compare and talk about the correspondance found in the left and right image, using the harris features in part C and the SIFT descriptors in part D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def drawMatches(imageA, imageB, kpsA, kpsB, matches, status):\n",
    "    # initialize the output visualization image\n",
    "    (hA, wA) = imageA.shape[:2]\n",
    "    (hB, wB) = imageB.shape[:2]\n",
    "    vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\")\n",
    "    vis[0:hA, 0:wA] = imageA\n",
    "    vis[0:hB, wA:] = imageB\n",
    "\n",
    "    # loop over the matches\n",
    "    for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "        # only process the match if the keypoint was successfully\n",
    "        # matched\n",
    "        if s == 1:\n",
    "            # draw the match\n",
    "            ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1]))\n",
    "            ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1]))\n",
    "            cv2.line(vis, ptA, ptB, (0, 255, 0), 1)\n",
    "    # return the visualization\n",
    "    return vis\n",
    "\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "img = ... # read the test image in part A\n",
    "\n",
    "gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "# use sift.detect to detect features in the images\n",
    "kp = ...\n",
    "\n",
    "img_kps = cv2.drawKeypoints(gray,kp,None,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "# plot the image with feature points marked out\n",
    "# Compare it with result in part A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ TODO ###############################################\n",
    "img1 = ... # read left image\n",
    "img2 = ... # read right iamge\n",
    "\n",
    "gray1= cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "gray2= cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "# use sift.detect to detect features in the images\n",
    "kp1 = ...\n",
    "kp2 = ...\n",
    "\n",
    "img1_kps = cv2.drawKeypoints(gray1,kp1,None,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2_kps = cv2.drawKeypoints(gray2,kp2,None,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "# use sift.compute to generate sift descriptors\n",
    "(kp1, features1) = ...\n",
    "(kp2, features2) = ...\n",
    "\n",
    "\n",
    "kp1 = np.float32([kp.pt for kp in kp1])\n",
    "kp2 = np.float32([kp.pt for kp in kp2])\n",
    "\n",
    "matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "################################################ TODO ###############################################\n",
    "# use knnMatch function in matcher to find corresonding features\n",
    "rawMatches = ...\n",
    "matches = []\n",
    "\n",
    "for m in rawMatches:\n",
    "    # ensure the distance is within a certain ratio of each\n",
    "    # other (i.e. Lowe's ratio test)\n",
    "    ################################################ TODO ###############################################\n",
    "    if len(m) == 2 and ...: # complete the if statement.test the distance between points. use m[0].distance and m[1].distance\n",
    "        matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "\n",
    "ptsA = np.float32([kp1[i] for (_,i) in matches])\n",
    "ptsB = np.float32([kp2[i] for (i,_) in matches])\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "(H, status) = cv2.findHomography(...) # fill in the parameters \n",
    "result = cv2.warpPerspective(...)# fill in the arguments to warp the second image to fit the first image.\n",
    "result[0:img2.shape[0], 0:img2.shape[1]] = img1\n",
    "\n",
    "vis = drawMatches(img1,img2,kp1,kp2,matches,status)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1_kps)\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2_kps)\n",
    "plt.title('images with keypoints')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(vis)\n",
    "plt.title('one to one correspondance between images')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1)\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "plt.imshow(result)\n",
    "plt.title('stitched image')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
